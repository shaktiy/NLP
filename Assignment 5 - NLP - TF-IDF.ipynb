{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5864eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import tqdm\n",
    "import re\n",
    "import string\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42c1fa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Shaktiy\\Desktop\\PyML\\PythonPracticeTextFiles\\Reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6fd1c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy = df.iloc[:500,:][['Id', 'Text']]\n",
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a8c17fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
      "======================================================================\n",
      "This wasn't in stock the last time I looked. I had to go to the Vermont Country Store in Weston to find it along with a jaw harp, Cranberry Horseradish Sauce, Fartless Black Bean Salsa, Apple Cider Jelly, Newton's Cradle Art in Motion and the staple Vermont Maple Syrup.<br /><br />Back to the Ass Kickin Peanuts. They are hot. They will activate the perspiration glands behind your ears and under your arms. It requires a beverage as advertised, a glass of very cold milk, and a box of Kleenex since it will make your nose run. They look like ordinary peanuts which is already giving me ideas for work. I suspect that some people have been hitting my goodies in my absence, especially my colleague Greg. I'm going to take this to work at earliest opportunity and empty the contents of this can into an ordinary Planters Peanuts can, and then see whose crying or whose nose is running when I return.<br /><br />The can should be shaken to ensure the spices are evenly distributed. It is important to wash your hands after consumption and not touch the eyes.<br /><br />You'll go nuts over these Ass-Kickin' Peanuts.<br /><br />P.S. I'm not sharing the peanuts, not deliberately, and I'll probably give Greg the jaw harp for Christmas. He'll be so insulted.\n",
      "======================================================================\n",
      "These Nature Valley Nut Lovers Variety Pack was perfect. Although I wasn't sure about the peanut butter bar, it was excellent. I loved the Roasted Almond, Roasted Pecan and the Peanut Butter.<br /><br />I will definately buy from this seller again. Quick shipping and very fresh.\n",
      "======================================================================\n",
      "I was buying Quaker Oats granola bars but these Nature Valley chewy bars are better tasting, they make a great snack when I'm on the go. Chocolate, peanuts and raisins...it doesn't get any better.\n",
      "======================================================================\n",
      "If you are a peanut lover, these are for you. Much larger than cocktail peanuts. Six people on my Christmas gift list ask for these every year!!\n",
      "======================================================================\n",
      "I bought these for my boyfriend who really likes honey roasted pecans in salads that I have at a couple of restaurants. Plus, he likes nuts, pecans, almonds peanuts. He loved these. I actually ordered 3 cans instead of the gift tin, so he could take one to work, or his place, and stay fresh longer. Although I don't think freshness will be an issue, they won't last long......lol\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in df_copy['Text']:\n",
    "    if 'peanut' in i.lower():\n",
    "        print(i)\n",
    "        print(\"======================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f6488",
   "metadata": {},
   "source": [
    "### Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4bcabd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_txt = re.compile('<.*?>')\n",
    "def clean_html(text):\n",
    "    cleantext = re.sub(clean_txt, ' ', text)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "21c406c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataCleaning(corpus):\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    final = []\n",
    "\n",
    "    for line in tqdm.tqdm(corpus):\n",
    "        line = clean_html(line)\n",
    "        line = line.lower() # Converts text to lower case\n",
    "        line = clean_html(line) # remove html\n",
    "        line = ' '.join([wrd for wrd in line.split() if wrd not in stopwords.words('english')]) #remove stopwords\n",
    "        line = ' '.join([wrd for wrd in line.split() if wrd.isalnum()]) # remove punc and other words that are n ot alpha numeric\n",
    "        line =  ' '.join([ps.stem(wrd) for wrd in line.split()])\n",
    "        final.append(line)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e680eefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:08<00:00, 56.45it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = DataCleaning(df_copy['Text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b3c7fa",
   "metadata": {},
   "source": [
    "### Getting Unique Words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef82840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(corpus):\n",
    "    unique_words = []\n",
    "    for i in corpus:\n",
    "        unique_words.extend(i.split())\n",
    "    unique_words = set(unique_words)\n",
    "    return(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e552a6",
   "metadata": {},
   "source": [
    "### IDF Calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "864e911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(corpus):\n",
    "    idf_dict={}\n",
    "    N=len(corpus)\n",
    "    for i in unque_words:\n",
    "        count=0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count=count+1\n",
    "        idf_dict[i]=(math.log((N)/(count)))\n",
    "    return idf_dict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "402c192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_dict = IDF(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cce10938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.912023005428146"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_dict['tasti']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5792a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unque_words = unique_words(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6825b",
   "metadata": {},
   "source": [
    "### TF-IDF function [Calculation for given corpus and unique word ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0e233624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(corpus, unque_words):\n",
    "    final = []\n",
    "    for sen in corpus:\n",
    "        inter = []\n",
    "        for unq in unque_words:\n",
    "            cnt = sen.split().count(unq)\n",
    "            tf_idf = cnt*idf_dict[unq]\n",
    "            inter.append(tf_idf)\n",
    "        final.append(inter)\n",
    "    return(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b0678",
   "metadata": {},
   "source": [
    "### TF-IDF function [Given new review in corpus unique word] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "518cca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDf_Review(corpus, unque_words, rev):\n",
    "    final = []\n",
    "    for wrd in unque_words:\n",
    "        if wrd in rev.split():\n",
    "            tfidf = rev.count(wrd)*idf_dict[wrd]\n",
    "            final.append(tfidf)   \n",
    "        else:\n",
    "            final.append(0)\n",
    "    return final    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5569196",
   "metadata": {},
   "source": [
    "### Dataframe for TF-IDF  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "69d1b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uq = unique_words(corpus)\n",
    "Final = TF_IDF(corpus, uq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d90ae221",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.DataFrame(Final, columns = uq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3b774e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>savora</th>\n",
       "      <th>sign</th>\n",
       "      <th>eat</th>\n",
       "      <th>regularli</th>\n",
       "      <th>content</th>\n",
       "      <th>healthiest</th>\n",
       "      <th>act</th>\n",
       "      <th>albanes</th>\n",
       "      <th>bake</th>\n",
       "      <th>nescaf</th>\n",
       "      <th>...</th>\n",
       "      <th>fat</th>\n",
       "      <th>hotlin</th>\n",
       "      <th>insid</th>\n",
       "      <th>thin</th>\n",
       "      <th>around</th>\n",
       "      <th>popper</th>\n",
       "      <th>amo</th>\n",
       "      <th>role</th>\n",
       "      <th>canning</th>\n",
       "      <th>solomon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.442019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2491 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   savora  sign  eat  regularli  content  healthiest  act  albanes  bake  \\\n",
       "0     0.0   0.0  0.0        0.0      0.0         0.0  0.0      0.0   0.0   \n",
       "1     0.0   0.0  0.0        0.0      0.0         0.0  0.0      0.0   0.0   \n",
       "2     0.0   0.0  0.0        0.0      0.0         0.0  0.0      0.0   0.0   \n",
       "3     0.0   0.0  0.0        0.0      0.0         0.0  0.0      0.0   0.0   \n",
       "4     0.0   0.0  0.0        0.0      0.0         0.0  0.0      0.0   0.0   \n",
       "\n",
       "   nescaf  ...  fat  hotlin  insid  thin    around  popper  amo  role  \\\n",
       "0     0.0  ...  0.0     0.0    0.0   0.0  0.000000     0.0  0.0   0.0   \n",
       "1     0.0  ...  0.0     0.0    0.0   0.0  0.000000     0.0  0.0   0.0   \n",
       "2     0.0  ...  0.0     0.0    0.0   0.0  3.442019     0.0  0.0   0.0   \n",
       "3     0.0  ...  0.0     0.0    0.0   0.0  0.000000     0.0  0.0   0.0   \n",
       "4     0.0  ...  0.0     0.0    0.0   0.0  0.000000     0.0  0.0   0.0   \n",
       "\n",
       "   canning  solomon  \n",
       "0      0.0      0.0  \n",
       "1      0.0      0.0  \n",
       "2      0.0      0.0  \n",
       "3      0.0      0.0  \n",
       "4      0.0      0.0  \n",
       "\n",
       "[5 rows x 2491 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d0957",
   "metadata": {},
   "source": [
    "### Vector using TF-IDF Vectorizer to validate if values calculated from above python code from scratch is equal to the one we got directly using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4663568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ec7da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2c7900cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 2477), 2477)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eab9a5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 1000, 0.32), (9, 500, 0.53))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10,1000, 0.32), (9,500,0.53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9b3de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sk = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cab33402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>100ml</th>\n",
       "      <th>10lb</th>\n",
       "      <th>11</th>\n",
       "      <th>110</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>1300watt</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>yr</th>\n",
       "      <th>yucki</th>\n",
       "      <th>yummi</th>\n",
       "      <th>zen</th>\n",
       "      <th>zest</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.175297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2477 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10  100  1000  100ml  10lb   11  110   12   13  1300watt  ...  york  \\\n",
       "0  0.0  0.0   0.0    0.0   0.0  0.0  0.0  0.0  0.0       0.0  ...   0.0   \n",
       "1  0.0  0.0   0.0    0.0   0.0  0.0  0.0  0.0  0.0       0.0  ...   0.0   \n",
       "2  0.0  0.0   0.0    0.0   0.0  0.0  0.0  0.0  0.0       0.0  ...   0.0   \n",
       "3  0.0  0.0   0.0    0.0   0.0  0.0  0.0  0.0  0.0       0.0  ...   0.0   \n",
       "4  0.0  0.0   0.0    0.0   0.0  0.0  0.0  0.0  0.0       0.0  ...   0.0   \n",
       "\n",
       "   young  younger  youngest   yr  yucki     yummi  zen  zest  zip  \n",
       "0    0.0      0.0       0.0  0.0    0.0  0.000000  0.0   0.0  0.0  \n",
       "1    0.0      0.0       0.0  0.0    0.0  0.000000  0.0   0.0  0.0  \n",
       "2    0.0      0.0       0.0  0.0    0.0  0.175297  0.0   0.0  0.0  \n",
       "3    0.0      0.0       0.0  0.0    0.0  0.000000  0.0   0.0  0.0  \n",
       "4    0.0      0.0       0.0  0.0    0.0  0.306047  0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 2477 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_sk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f0f92d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(d_sk['young']==f['young']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047ad8c",
   "metadata": {},
   "source": [
    "### To see if two reviews are same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fffb880b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 50.00it/s]\n"
     ]
    }
   ],
   "source": [
    "SampleReview = \"Peanut butter is very cheap and it is very tasty\"\n",
    "\n",
    "f = DataCleaning([SampleReview])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a5d1de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peanut butter cheap tasti']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0d895952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'peanut butter cheap tasti'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev = f[0]\n",
    "rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "69651b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_wrds = unique_words(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9924846",
   "metadata": {},
   "source": [
    "### Cosine Similarity to find out similar reviews to a sample review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "108207d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CosineSimilarity(a, vectorB, corpus):\n",
    "    coslist = []\n",
    "    for i in a:\n",
    "        vectorA = i\n",
    "        cos = np.dot(vectorA,vectorB)/(np.sqrt(np.dot(vectorA,vectorA))*np.sqrt(np.dot(vectorB,vectorB)))\n",
    "        coslist.append(cos)\n",
    "    similarReview = corpus[np.argmax(coslist)]    \n",
    "    return similarReview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "13e17605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natur valley nut lover varieti pack although sure peanut butter love roast roast pecan peanut defin buy seller quick ship'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = TF_IDF(corpus, unque_words)\n",
    "vectorB = TF_IDf_Review(corpus, unque_words, rev)\n",
    "CosineSimilarity(a, vectorB, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5096ee4",
   "metadata": {},
   "source": [
    "### Euclidean Distance to find out similar reviews to sample review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "efc99638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanSimilarity(a, vectorB, corpus):\n",
    "    euclist = []\n",
    "    for i in a:\n",
    "        vectorA = i\n",
    "        euclideandist = np.linalg.norm(np.array(vectorA)-np.array(vectorB))\n",
    "        euclist.append(euclideandist)\n",
    "    similarReview = corpus[np.argmin(euclist)]    \n",
    "    return similarReview    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "95377599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'would go way buy'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = TF_IDF(corpus, unque_words)\n",
    "vectorB = TF_IDf_Review(corpus, unque_words, rev)\n",
    "EuclideanSimilarity(a, vectorB, corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
